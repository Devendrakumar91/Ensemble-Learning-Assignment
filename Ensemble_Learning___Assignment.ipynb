{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is Ensemble Learning in machine learning? Explain the key idea behind it.\n",
        "\n",
        "Ans:-\n",
        "Ensemble Learning is a machine learning paradigm where multiple models (called base learners) are combined to solve the same problem, with the goal of achieving better performance, stability, and generalization than any single model alone.\n",
        "\n",
        "Key Idea Behind Ensemble Learning\n",
        "\n",
        "- “Many weak learners together can form a strong learner.”\n",
        "\n",
        "Instead of relying on one model:\n",
        "\n",
        "- Train several different models\n",
        "\n",
        "- Combine their predictions intelligently\n",
        "\n",
        "- Reduce individual model errors\n",
        "\n",
        "This works because different models tend to make different mistakes, and combining them helps cancel out errors.\n",
        "\n",
        "Why Ensemble Learning Works\n",
        "\n",
        "Ensembles improve performance by addressing:\n",
        "- Bias → underfitting\n",
        "- Variance → overfitting\n",
        "- Noise sensitivity\n"
      ],
      "metadata": {
        "id": "6alr6K1atkML"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: What is the difference between Bagging and Boosting?\n",
        "\n",
        "Ans:- Difference Between Bagging and Boosting\n",
        "\n",
        "Both Bagging and Boosting are ensemble learning techniques, but they differ fundamentally in how models are trained and how errors are handled.\n",
        "\n",
        "1. Bagging (Bootstrap Aggregating)\n",
        "\n",
        "Key Idea\n",
        "\n",
        "- Reduce variance by training models independently on different random subsets of data.\n",
        "\n",
        "How It Works\n",
        "\n",
        "1. Create multiple bootstrap samples (sampling with replacement).\n",
        "2. Train a base model on each sample independently.\n",
        "3. Combine predictions by:\n",
        "\n",
        "- Majority voting (classification)\n",
        "- Averaging (regression)\n",
        "\n",
        "Characteristics\n",
        "\n",
        "- Models are trained in parallel\n",
        "- Each model has equal weight\n",
        "- Does not focus on difficult samples\n",
        "\n",
        "Best Suited For\n",
        "\n",
        "- High-variance models\n",
        "- Models prone to overfitting\n",
        "\n",
        "2. Boosting\n",
        "\n",
        "Key Idea\n",
        "\n",
        "- Reduce bias by training models sequentially, focusing on previous errors.\n",
        "\n",
        "How It Works\n",
        "\n",
        "1. Train a weak learner on the dataset.\n",
        "2. Increase weights of misclassified samples.\n",
        "3. Train the next model emphasizing these difficult samples.\n",
        "4. Combine models using weighted voting\n",
        "\n",
        "Characteristics\n",
        "\n",
        "- Models are trained sequentially\n",
        "- Each model has a different weight\n",
        "- Strongly focuses on hard-to-classify points\n",
        "\n",
        "Best Suited For\n",
        "\n",
        "- High-bias models\n",
        "- Complex patterns\n",
        "\n",
        "Examples\n",
        "\n",
        "- AdaBoost\n",
        "- Gradient Boosting\n",
        "- XGBoost / LightGBM\n",
        "\n"
      ],
      "metadata": {
        "id": "GkrNL8HftkJR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?\n",
        "\n",
        "Ans:-\n",
        "Bootstrap sampling is a resampling technique where multiple datasets are created by randomly sampling from the original dataset with replacement.\n",
        "\n",
        "Each bootstrap sample has the same size as the original dataset, but may contain duplicate records and leave out some original samples.\n",
        "\n",
        "How Bootstrap Sampling Works\n",
        "\n",
        "- Given a dataset of size N:\n",
        "- Randomly sample N points with replacement\n",
        "- Some observations appear multiple times\n",
        "- Some observations are not selected at all\n",
        "\n",
        "On average:\n",
        "\n",
        "- ~63.2% unique samples are included\n",
        "- ~36.8% samples are left out (out-of-bag samples)\n",
        "\n",
        "Role of Bootstrap Sampling in Bagging\n",
        "1. Creates Diversity Among Models\n",
        "\n",
        "- Each model sees a different version of the data\n",
        "- Leads to decorrelated models\n",
        "\n",
        "Why important?\n",
        "- Ensemble learning only works if models make different errors.\n",
        "\n",
        "2. Reduces Variance\n",
        "\n",
        "- Individual models (e.g., decision trees) have high variance\n",
        "- Averaging predictions from multiple bootstrap-trained models stabilizes results\n",
        "\n",
        "3. Enables Parallel Training\n",
        "\n",
        "- Bootstrap samples are independent\n",
        "- Models can be trained in parallel\n",
        "\n",
        "Bootstrap Sampling in Random Forest\n",
        "\n",
        "Random Forest applies two levels of randomness:\n",
        "\n",
        "1. Data Randomness (Bootstrap Sampling)\n",
        "\n",
        "- Each decision tree is trained on a bootstrap sample\n",
        "\n",
        "2. Feature Randomness\n",
        "\n",
        "- At each split, a random subset of features is considered\n",
        "\n",
        "- This double randomness makes trees less correlated."
      ],
      "metadata": {
        "id": "cEAOim9ctkGb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: What are Out-of-Bag (OOB) samples and how is OOB score used to evaluate ensemble models?\n",
        "\n",
        "Ans- Out-of-Bag (OOB) samples arise naturally in Bagging-based ensemble methods such as Random Forest due to bootstrap sampling\n",
        "\n",
        "What Are Out-of-Bag (OOB) Samples?\n",
        "\n",
        "- In bootstrap sampling, each model is trained on a dataset created by sampling with replacement from the original dataset.\n",
        "\n",
        "As a result:\n",
        "\n",
        "- Some samples appear multiple times\n",
        "- Some samples are not selected at all\n",
        "\n",
        "The samples not used to train a particular model are called Out-of-Bag (OOB) samples for that model.\n",
        "\n",
        "- Key Fact\n",
        "\n",
        "- On average, ~36.8% of the original data becomes OOB for each model.\n",
        "\n",
        "How OOB Score Is Used to Evaluate Ensemble Models\n",
        "Step-by-Step Process\n",
        "\n",
        "1. For each data point:\n",
        "\n",
        "- Identify all trees where this data point was OOB\n",
        "\n",
        "2. Make predictions using only those trees\n",
        "\n",
        "3. Aggregate predictions:\n",
        "\n",
        "- Majority vote (classification)\n",
        "- Mean (regression)\n",
        "\n",
        "4. Compare predicted value with the true label\n",
        "\n",
        "OOB Score\n",
        "\n",
        "- The OOB score is the overall accuracy (or R² for regression) computed using only OOB predictions.\n",
        "- It serves as an internal validation metric.\n",
        "\n",
        "Why OOB Score Is Useful\n",
        "1. No Need for a Separate Validation Set\n",
        "\n",
        "- Saves data (important when data is limited)\n",
        "- Especially valuable in biomedical and small-sample problems\n",
        "\n",
        "2. Unbiased Performance Estimate\n",
        "\n",
        "- Each sample is evaluated on models that never saw it during training\n",
        "- Similar to cross-validation\n",
        "\n",
        "3. Computationally Efficient\n",
        "\n",
        "- Comes “for free” during training\n",
        "- No extra resampling required\n"
      ],
      "metadata": {
        "id": "ZQJjC7o4tkDx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: Compare feature importance analysis in a single Decision Tree vs. a Random Forest.\n",
        "\n",
        "Ans:- Feature importance explains which input features most influence a model’s predictions.\n",
        "\n",
        "Both Decision Trees and Random Forests provide feature importance, but they differ significantly in stability, reliability, and interpretation.\n",
        "\n",
        "1. Feature Importance in a Single Decision Tree\n",
        "How It Is Computed\n",
        "\n",
        "- Based on reduction in impurity (Gini, Entropy, or MSE)\n",
        "\n",
        "- Features used near the root usually get higher importance\n",
        "\n",
        "- Importance is calculated from one tree only\n",
        "\n",
        "Characteristics\n",
        "\n",
        "- Highly sensitive to data variations\n",
        "\n",
        "- Can change drastically with small data changes\n",
        "\n",
        "- Prone to overfitting\n",
        "\n",
        "- Strong bias toward features with:\n",
        "\n",
        "- Many unique values\n",
        "\n",
        "- Continuous variables\n",
        "\n",
        "2. Feature Importance in Random Forest\n",
        "How It Is Computed\n",
        "\n",
        "- Average of impurity reduction across many trees\n",
        "\n",
        "- Each tree sees:\n",
        "\n",
        "- A different bootstrap sample\n",
        "\n",
        "- A random subset of features at each split\n",
        "\n",
        "Characteristics\n",
        "\n",
        "- Much more stable than a single tree\n",
        "\n",
        "- Less sensitive to noise\n",
        "\n",
        "- Captures global feature relevance\n",
        "\n",
        "- Still biased toward high-cardinality features (for impurity-based importance)\n",
        "\n"
      ],
      "metadata": {
        "id": "IpveEdoKtkAu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: Write a Python program to:\n",
        "● Load the Breast Cancer dataset using\n",
        "sklearn.datasets.load_breast_cancer()\n",
        "● Train a Random Forest Classifier\n",
        "● Print the top 5 most important features based on feature importance scores.\n"
      ],
      "metadata": {
        "id": "Z6ZDGGqRtj9V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LoEeSn41tg5Z"
      },
      "outputs": [],
      "source": [
        "# Question 6: Write a Python program to:\n",
        "# Load the Breast Cancer dataset using sklearn.datasets.load_breast_cancer()\n",
        "# Train a Random Forest Classifier\n",
        "# Print the top 5 most important features based on feature importance scores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf81fce9",
        "outputId": "f02e6b65-df9d-459c-de64-01232a42f81b"
      },
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 1. Load the Breast Cancer dataset\n",
        "bcs = load_breast_cancer()\n",
        "X = bcs.data\n",
        "y = bcs.target\n",
        "feature_names = bcs.feature_names\n",
        "\n",
        "print(\"Dataset loaded successfully.\")\n",
        "print(f\"Number of samples: {X.shape[0]}\")\n",
        "print(f\"Number of features: {X.shape[1]}\")\n",
        "\n",
        "# 2. Train a Random Forest Classifier\n",
        "# Using a random state for reproducibility\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_classifier.fit(X, y)\n",
        "\n",
        "print(\"\\nRandom Forest Classifier trained successfully.\")\n",
        "\n",
        "# 3. Print the top 5 most important features based on feature importance scores.\n",
        "# Get feature importances from the trained model\n",
        "importances = rf_classifier.feature_importances_\n",
        "\n",
        "# Create a pandas Series for easier sorting and display\n",
        "feature_importances = pd.Series(importances, index=feature_names)\n",
        "\n",
        "# Sort the features by importance in descending order and get the top 5\n",
        "top_5_features = feature_importances.nlargest(5)\n",
        "\n",
        "print(\"\\nTop 5 most important features:\")\n",
        "print(top_5_features)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded successfully.\n",
            "Number of samples: 569\n",
            "Number of features: 30\n",
            "\n",
            "Random Forest Classifier trained successfully.\n",
            "\n",
            "Top 5 most important features:\n",
            "worst area              0.139357\n",
            "worst concave points    0.132225\n",
            "mean concave points     0.107046\n",
            "worst radius            0.082848\n",
            "worst perimeter         0.080850\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 7: Write a Python program to:\n",
        "# Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "# Evaluate its accuracy and compare with a single Decision Tree\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1c3752LS4Qfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ef6cc3dd",
        "outputId": "c5cefd35-a42b-43e7-b927-4104117963a5"
      },
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "print(\"Iris dataset loaded successfully.\")\n",
        "print(f\"Number of samples: {X.shape[0]}\")\n",
        "print(f\"Number of features: {X.shape[1]}\")\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "print(\"\\nData split into training and testing sets.\")\n",
        "\n",
        "# 2. Train a single Decision Tree Classifier\n",
        "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
        "dt_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and evaluate accuracy for single Decision Tree\n",
        "y_pred_dt = dt_classifier.predict(X_test)\n",
        "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
        "\n",
        "print(\"\\nSingle Decision Tree Classifier trained successfully.\")\n",
        "print(f\"Accuracy of single Decision Tree: {accuracy_dt:.4f}\")\n",
        "\n",
        "# 3. Train a Bagging Classifier using Decision Trees\n",
        "# Base estimator is a Decision Tree. We'll use 10 base estimators (n_estimators=10)\n",
        "bag_classifier = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(random_state=42),\n",
        "    n_estimators=10,\n",
        "    random_state=42,\n",
        "    bootstrap=True, # Bootstrap sampling is enabled by default\n",
        "    n_jobs=-1 # Use all available CPU cores\n",
        ")\n",
        "bag_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and evaluate accuracy for Bagging Classifier\n",
        "y_pred_bag = bag_classifier.predict(X_test)\n",
        "accuracy_bag = accuracy_score(y_test, y_pred_bag)\n",
        "\n",
        "print(\"\\nBagging Classifier trained successfully.\")\n",
        "print(f\"Accuracy of Bagging Classifier: {accuracy_bag:.4f}\")\n",
        "\n",
        "# 4. Compare the accuracies\n",
        "print(\"\\n--- Comparison ---\")\n",
        "if accuracy_bag > accuracy_dt:\n",
        "    print(f\"The Bagging Classifier (Accuracy: {accuracy_bag:.4f}) performed better than the single Decision Tree (Accuracy: {accuracy_dt:.4f}).\")\n",
        "elif accuracy_bag < accuracy_dt:\n",
        "    print(f\"The single Decision Tree (Accuracy: {accuracy_dt:.4f}) performed better than the Bagging Classifier (Accuracy: {accuracy_bag:.4f}).\")\n",
        "else:\n",
        "    print(f\"Both models performed equally well (Accuracy: {accuracy_bag:.4f}).\")\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iris dataset loaded successfully.\n",
            "Number of samples: 150\n",
            "Number of features: 4\n",
            "\n",
            "Data split into training and testing sets.\n",
            "\n",
            "Single Decision Tree Classifier trained successfully.\n",
            "Accuracy of single Decision Tree: 1.0000\n",
            "\n",
            "Bagging Classifier trained successfully.\n",
            "Accuracy of Bagging Classifier: 1.0000\n",
            "\n",
            "--- Comparison ---\n",
            "Both models performed equally well (Accuracy: 1.0000).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 8: Write a Python program to:\n",
        "# Train a Random Forest Classifier\n",
        "# Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "# Print the best parameters and final accuracy\n"
      ],
      "metadata": {
        "id": "VyLFT-tW4eSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34730cd7",
        "outputId": "79510c9d-9631-483a-f797-45a1dd03fc46"
      },
      "source": [
        "# Import necessary libraries for Random Forest and GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print(\"Assuming Iris dataset (X_train, X_test, y_train, y_test) is already loaded from previous steps.\")\n",
        "\n",
        "# 1. Initialize a Random Forest Classifier\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# 2. Define the hyperparameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_depth': [None, 5, 10, 15]\n",
        "}\n",
        "\n",
        "# 3. Set up GridSearchCV\n",
        "# cv=5 means 5-fold cross-validation\n",
        "# verbose=1 shows some progress messages\n",
        "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=1, scoring='accuracy')\n",
        "\n",
        "# Fit GridSearchCV to the training data\n",
        "print(\"\\nStarting GridSearchCV to find the best hyperparameters...\")\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"GridSearchCV completed.\")\n",
        "\n",
        "# 4. Print the best parameters and best score\n",
        "best_params = grid_search.best_params_\n",
        "best_score = grid_search.best_score_\n",
        "\n",
        "print(f\"\\nBest Hyperparameters: {best_params}\")\n",
        "print(f\"Best Cross-validation Accuracy: {best_score:.4f}\")\n",
        "\n",
        "# 5. Evaluate the model with the best parameters on the test set\n",
        "best_rf_model = grid_search.best_estimator_\n",
        "y_pred_tuned = best_rf_model.predict(X_test)\n",
        "final_accuracy = accuracy_score(y_test, y_pred_tuned)\n",
        "\n",
        "print(f\"Final Accuracy on Test Set with Best Parameters: {final_accuracy:.4f}\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assuming Iris dataset (X_train, X_test, y_train, y_test) is already loaded from previous steps.\n",
            "\n",
            "Starting GridSearchCV to find the best hyperparameters...\n",
            "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
            "GridSearchCV completed.\n",
            "\n",
            "Best Hyperparameters: {'max_depth': None, 'n_estimators': 100}\n",
            "Best Cross-validation Accuracy: 0.9429\n",
            "Final Accuracy on Test Set with Best Parameters: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 9: Write a Python program to:\n",
        "# Train a Bagging Regressor and a Random Forest Regressor on the California Housing dataset\n",
        "# Compare their Mean Squared Errors (MSE)\n"
      ],
      "metadata": {
        "id": "93F3qlVT42dd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d54a2ff3",
        "outputId": "c5aa3d58-05c7-4592-8685-f9b1e6a52162"
      },
      "source": [
        "# Import Required Libraries\n",
        "import numpy as np\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "\n",
        "# Load the California Housing Dataset\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train–Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train a Bagging Regressor\n",
        "bagging_reg = BaggingRegressor(\n",
        "    estimator=DecisionTreeRegressor(),\n",
        "    n_estimators=100,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "bagging_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred_bagging = bagging_reg.predict(X_test)\n",
        "\n",
        "# MSE\n",
        "mse_bagging = mean_squared_error(y_test, y_pred_bagging)\n",
        "print(\"Bagging Regressor MSE:\", mse_bagging)\n",
        "\n",
        "# Train a Random Forest Regressor\n",
        "\n",
        "rf_reg = RandomForestRegressor(\n",
        "    n_estimators=100,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "rf_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred_rf = rf_reg.predict(X_test)\n",
        "\n",
        "# MSE\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "print(\"Random Forest Regressor MSE:\", mse_rf)\n",
        "\n",
        "# Compare the Results\n",
        "print(\"\\nMSE Comparison:\")\n",
        "print(f\"Bagging Regressor MSE: {mse_bagging:.4f}\")\n",
        "print(f\"Random Forest Regressor MSE: {mse_rf:.4f}\")\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Regressor MSE: 0.25592438609899626\n",
            "Random Forest Regressor MSE: 0.2553684927247781\n",
            "\n",
            "MSE Comparison:\n",
            "Bagging Regressor MSE: 0.2559\n",
            "Random Forest Regressor MSE: 0.2554\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: You are working as a data scientist at a financial institution to predict loan default. You have access to customer demographic and transaction history data.\n",
        "You decide to use ensemble techniques to increase model performance.\n",
        "\n",
        "Explain your step-by-step approach to:\n",
        "- Choose between Bagging or Boosting\n",
        "- Handle overfitting\n",
        "- Select base models\n",
        "- Evaluate performance using cross-validation\n",
        "- Justify how ensemble learning improves decision-making in this real-world context.\n",
        "\n",
        "Ans:-\n",
        "Step-by-Step Ensemble Strategy for Loan Default Prediction\n",
        "\n",
        "Loan default prediction is a high-stakes, imbalanced, and noisy real-world problem. Ensemble learning improves accuracy, stability, and risk control, which are critical in finance.\n",
        "\n",
        "1. Choosing Between Bagging and Boosting\n",
        "\n",
        "Step 1: Understand Data Characteristics\n",
        "\n",
        "- Mixed features: demographics + transaction behavior\n",
        "- Non-linear relationships\n",
        "- Class imbalance (few defaults)\n",
        "- Risk of overfitting\n",
        "\n",
        "2. Handling Overfitting\n",
        "\n",
        "Techniques Used\n",
        "\n",
        "A. Model-Level Controls\n",
        "- Random Forest:\n",
        "- Limit tree depth\n",
        "- Minimum samples per leaf\n",
        "\n",
        "Boosting:\n",
        "- Learning rate\n",
        "- Early stopping\n",
        "- Subsampling\n",
        "\n",
        "B. Data-Level Controls\n",
        "\n",
        "- Feature scaling (where required)\n",
        "- Handle class imbalance:\n",
        "- Class weights\n",
        "- SMOTE (if justified)\n",
        "\n",
        "C. Validation Controls\n",
        "\n",
        "- Stratified cross-validation\n",
        "- Out-of-Bag (OOB) error for bagging\n",
        "\n",
        "3. Selecting Base Models\n",
        "\n",
        "Why Tree-Based Models?\n",
        "\n",
        "- Handle non-linearity\n",
        "- Work well with mixed data types\n",
        "- No strict assumptions\n",
        "\n",
        "4. Evaluating Performance Using Cross-Validation\n",
        "* Why Accuracy Is Not Enough\n",
        "- Loan default is imbalanced, so we focus on risk-aware metrics.\n"
      ],
      "metadata": {
        "id": "rLLGEcUx5rJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Import Required Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
        "from sklearn.metrics import (\n",
        "    roc_auc_score, classification_report, confusion_matrix\n",
        ")\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "\n",
        "# Load / Simulate Loan Default Dataset\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# Simulated dataset\n",
        "n_samples = 5000\n",
        "X = pd.DataFrame({\n",
        "    \"age\": np.random.randint(21, 65, n_samples),\n",
        "    \"income\": np.random.normal(50000, 15000, n_samples),\n",
        "    \"loan_amount\": np.random.normal(20000, 8000, n_samples),\n",
        "    \"credit_score\": np.random.normal(650, 70, n_samples),\n",
        "    \"transaction_count\": np.random.poisson(30, n_samples),\n",
        "    \"late_payments\": np.random.poisson(2, n_samples)\n",
        "})\n",
        "\n",
        "# Target: 1 = Default, 0 = No Default (imbalanced)\n",
        "y = np.random.binomial(1, 0.18, n_samples)\n",
        "\n",
        "# Train–Test Split (Stratified)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.25,\n",
        "    stratify=y,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Baseline Model (Single Decision Tree)\n",
        "\n",
        "dt_pipeline = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"classifier\", DecisionTreeClassifier(random_state=42))\n",
        "])\n",
        "\n",
        "dt_pipeline.fit(X_train, y_train)\n",
        "y_pred_dt = dt_pipeline.predict(X_test)\n",
        "\n",
        "print(\"Single Decision Tree Performance:\")\n",
        "print(classification_report(y_test, y_pred_dt))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred_dt))\n",
        "\n",
        "# Bagging Approach – Random Forest\n",
        "\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=300,\n",
        "    max_depth=8,\n",
        "    min_samples_leaf=30,\n",
        "    class_weight=\"balanced\",\n",
        "    oob_score=True,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "rf_pred = rf_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"Random Forest ROC-AUC:\",\n",
        "      roc_auc_score(y_test, rf_pred))\n",
        "print(\"OOB Score:\", rf_model.oob_score_)\n",
        "\n",
        "# Boosting Approach – Gradient Boosting\n",
        "\n",
        "gb_model = GradientBoostingClassifier(\n",
        "    n_estimators=200,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=3,\n",
        "    subsample=0.8,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "gb_model.fit(X_train, y_train)\n",
        "\n",
        "gb_pred = gb_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"Gradient Boosting ROC-AUC:\",\n",
        "      roc_auc_score(y_test, gb_pred))\n",
        "\n",
        "# Cross-Validation Evaluation (Risk-Aware)\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "rf_cv_auc = cross_val_score(\n",
        "    rf_model, X_train, y_train,\n",
        "    scoring=\"roc_auc\",\n",
        "    cv=cv\n",
        ")\n",
        "\n",
        "gb_cv_auc = cross_val_score(\n",
        "    gb_model, X_train, y_train,\n",
        "    scoring=\"roc_auc\",\n",
        "    cv=cv\n",
        ")\n",
        "\n",
        "print(\"RF CV ROC-AUC:\", rf_cv_auc.mean())\n",
        "print(\"GB CV ROC-AUC:\", gb_cv_auc.mean())\n",
        "\n",
        "# Final Model Evaluation (Confusion Matrix)\n",
        "\n",
        "threshold = 0.5\n",
        "final_preds = (gb_pred > threshold).astype(int)\n",
        "\n",
        "print(\"Confusion Matrix:\\n\",\n",
        "      confusion_matrix(y_test, final_preds))\n",
        "\n",
        "print(\"\\nClassification Report:\\n\",\n",
        "      classification_report(y_test, final_preds))\n",
        "\n",
        "# Feature Importance (Explainability)\n",
        "\n",
        "importances = pd.Series(\n",
        "    rf_model.feature_importances_,\n",
        "    index=X.columns\n",
        ").sort_values(ascending=False)\n",
        "\n",
        "print(\"Feature Importance (Random Forest):\")\n",
        "print(importances)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4tsFJX08sAf",
        "outputId": "26b0db71-9369-4439-ade7-460bfbf4f028"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Single Decision Tree Performance:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.79      0.80      1018\n",
            "           1       0.17      0.19      0.18       232\n",
            "\n",
            "    accuracy                           0.68      1250\n",
            "   macro avg       0.49      0.49      0.49      1250\n",
            "weighted avg       0.69      0.68      0.69      1250\n",
            "\n",
            "Confusion Matrix:\n",
            "[[807 211]\n",
            " [188  44]]\n",
            "Random Forest ROC-AUC: 0.5000169365219158\n",
            "OOB Score: 0.6674666666666667\n",
            "Gradient Boosting ROC-AUC: 0.49721817627532006\n",
            "RF CV ROC-AUC: 0.4995066190982561\n",
            "GB CV ROC-AUC: 0.4934798197463907\n",
            "Confusion Matrix:\n",
            " [[1017    1]\n",
            " [ 232    0]]\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      1.00      0.90      1018\n",
            "           1       0.00      0.00      0.00       232\n",
            "\n",
            "    accuracy                           0.81      1250\n",
            "   macro avg       0.41      0.50      0.45      1250\n",
            "weighted avg       0.66      0.81      0.73      1250\n",
            "\n",
            "Feature Importance (Random Forest):\n",
            "loan_amount          0.223464\n",
            "credit_score         0.217984\n",
            "income               0.207460\n",
            "age                  0.161306\n",
            "transaction_count    0.130010\n",
            "late_payments        0.059776\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WA3OoWEr9pGR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}